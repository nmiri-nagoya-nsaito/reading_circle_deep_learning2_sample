{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 リカレントニューラルネットワーク(RNN)\n",
    "* これまでのニューラルネットワークはフィードフォワードと呼ばれるタイプのネットワーク\n",
    "    * フィードフォワードとは流れが一方向であるということ\n",
    "* フィードフォワードネットワークは単純で理解しやすいが，時系列データをうまく扱えない\n",
    "    * 時系列データの性質(パターン)を十分に学習することができないということ\n",
    "* リカレントネットワーク(RNN)はフィードフォワードネットワークの問題点を解決することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 確率と言語モデル\n",
    "### word2vecを確率の視点から眺める\n",
    "#### word2vecのCBOWモデルの復習\n",
    "$ w_1,w_2, ... ,w_T $ という単語の列で表されるコーパスを考える．\n",
    "\n",
    "t番目の単語を「ターゲット」とし，その前後の単語を「コンテキスト」として扱う．\n",
    "\n",
    "この時，CBOWモデルが行うことはコンテキストの$w_{t-1}$と$w_{t+1}$とから，ターゲットの$w_t$を推測すること．\n",
    "\n",
    "$w_{t-1}$と$w_{t+1}$が与えられた時にターゲットが$w_t$となる確率\n",
    "\n",
    "$$\n",
    "P(wt \\mid w_{t-1},w_{t+1})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 言語モデル\n",
    "\n",
    "### 5.1.3 CBOWモデルを言語モデルに？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 RNNとは\n",
    "### 5.2.1 循環するニューラルネットワーク\n",
    "### 5.2.2 ループの展開\n",
    "### 5.2.3 Backpropagation Through Time\n",
    "### 5.2.4 Truncated BPTT\n",
    "### 5.2.5 truncated BPTTのミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 RNNの実装\n",
    "### 5.3.1 RNNレイヤの実装\n",
    "### 5.3.2 Time RNNレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 時系列データを扱うレイヤの実装\n",
    "### 5.4.1 RNNLMの全体図\n",
    "### 5.4.2 Timeレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 RNNLMの学習と評価\n",
    "### 5.5.1 RNNLMの実装\n",
    "### 5.5.2 言語モデルの評価\n",
    "### 5.5.3 RNNLMの学習コード\n",
    "### 5.5.4 RNNLMのTrainerクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 まとめ\n",
    "* RNNはデータを循環させることで過去から現在，未来へとデータを継続して流す\n",
    "* 流すことによってRNNレイヤの内部に「隠れ状態」を記憶する能力を得る\n",
    "* RNNレイヤの仕組みを説明し，実装した\n",
    "* RNNを利用して言語モデルを作成した\n",
    "* 言語モデルは単語の羅列に対して確率を与える\n",
    "* 条件付き言語モデルはこれまでの単語の羅列から次に出現する単語の確率を算出する\n",
    "* モデルでRNNを利用したニューラルネットワークを構成することで理論的には長い時系列データであっても重要な情報をRNNの隠れ状態に記憶できる\n",
    "* 実際の問題ではうまく学習できないケースが多いため次節ではそれに変わるレイヤを見る\n",
    "\n",
    "* RNNはループする経路があり，それにより隠れ状態を内部に記憶できる\n",
    "* RNNのループ鶏肋を展開することで複数のRNNレイヤが繋がったネットワークと解釈でき，通常の誤差逆伝播法により学習できる(=BPTT)\n",
    "* 長い時系列データの学習ではブロックと呼ばれる適当な長さのデータのまとまりを作り，ブロック単位でBPTTによる学習を行う(=Truncated BPTT)\n",
    "* Truncated BPTTでは逆伝播のつながりのみを切断する\n",
    "* Truncated BPTTは順伝播のつながりは維持するため，データをシーケンシャルに与える必要がある\n",
    "* 言語モデルは単語の羅列を確率として解釈する\n",
    "* RNNレイヤを利用した条件付き言語モデルは（理論上，）それまでに登場した単語の情報を記憶することができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
