{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3章 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テーマ：単語の分散表現について\n",
    "* おさらい：「単語の分散表現（P67）」とは\n",
    "  * 単語の意味を的確に捉えた単語のベクトルによる表現\n",
    "  * 自然処理の分野での用語\n",
    "* 期待される性質\n",
    "  * コンパクトであること\n",
    "  * 正確に指定できること\n",
    "  * 関連性が判断しやすいこと\n",
    "  * 定量化しやすいこと\n",
    "  * 理にかなっていること\n",
    "* 表現形式：密な固定長ベクトル（多くが0でない実数値で表現されること）\n",
    "* 分散表現を得るための方法\n",
    "  * カウントベースの手法\n",
    "    * 周囲の単語の頻度によって単語を表現\n",
    "    * 単語の共起行列を作り，それを特異値分解(SVD)して密なベクトル表現を得る\n",
    "  * 推論ベースの手法\n",
    "    * 推論問題を繰り返し解くことで学習し，コンテキストから各単語の出現確率を出力するモデルを生成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 推論ベースの手法とニューラルネットワーク\n",
    "### 3.1.1 カウントベースの手法の問題点\n",
    "* 大規模なコーパスでは語彙数が巨大になり，共起行列も巨大になる\n",
    "  * 英語の語彙数は100万語を超えると言われる\n",
    "  * その場合の共起行列は100万×100万\n",
    "  * SVDは$O(n^3)$の計算量となるため，現実的ではない\n",
    "* 推論ベースの特徴\n",
    "  * ミニバッチで学習\n",
    "    * 大抵の計算機で計算が実施できる\n",
    "    * 並列化が容易で学習全体の高速化が可能\n",
    "  * 語彙に新しい単語を追加する場合でもパラメータの再学習が可能\n",
    "    * それまでの学習データを初期値として再学習して分散表現を更新できる\n",
    "  * 分散表現が単語の類似性に加えて複雑な単語間のパターンも捉えることができる\n",
    "    * 類推問題(例：king-man+woman=queen)を解くことができる\n",
    "    * カウントベースの手法では単語間の類似性が主\n",
    "  * 精度的にはカウントベース手法との優劣はつけられないとの報告がある\n",
    "    * [Improving Distributional Similarity\n",
    "with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016)\n",
    "\n",
    "### 3.1.2 推論ベースの手法の概要\n",
    "* 推論ベースの手法で行うこと：「推論問題」を解くこと，および，そのために学習すること\n",
    "  * 推論問題：周囲の単語（コンテキスト）が与えられた時にその間にどういう単語が出現するかを推論する\n",
    "* 推論問題を繰り返し解くことで，単語の出現パターンを学習する\n",
    "* 得られるモデルは，コンテキストを入力として与えた時に各単語の出現確率を出力するモデルとなる\n",
    "\n",
    "### 3.1.3 ニューラルネットワークにおける単語の処理方法\n",
    "* 単語を表現する方法の一つして「one-hot表現（one-hotベクトル）」がある\n",
    "  * one-hotの説明は省略\n",
    "* 手順\n",
    "  1. コーパスに含まれる単語にIDを付与する\n",
    "  1. IDからその単語に対するone-hot表現に変換する\n",
    "    * one-hotのベクトル長は語彙数(単語IDの数)に固定される\n",
    "    * コーパスの大きさによらない\n",
    "    * ニューラルネットワークの入力層のニューロン数はこのサイズになる\n",
    "\n",
    "#### 例（コーパス：\"You say goodbye and I say hello.\"）\n",
    "\n",
    "| word    |ID| one-hot |\n",
    "|---------|--|---------|\n",
    "| you     | 0| 1000000 |\n",
    "| say     | 1| 0100000 |\n",
    "| goodbye | 2| 0010000 |\n",
    "| and     | 3| 0001000 |\n",
    "| I       | 4| 0000100 |\n",
    "| hello   | 5| 0000010 |\n",
    "| .       | 6| 0000001 |\n",
    "\n",
    "#### 全結合層による表現\n",
    "\n",
    "* one-hotで表現した単語は，レイヤを重ねることで処理を行うことができる\n",
    "  * 例えばバイアスを用いない全結合層(単純に行列の積として表現される)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12119661 -1.25299626 -0.89137936]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 単語IDが0の単語を one-hot で表現し，全結合層によって変換する例\n",
    "# 今後ミニバッチを使うことを見越して c は2次元配列にしている\n",
    "c = np.array([[1,0,0,0,0,0,0]]) # 入力(one-hot)\n",
    "W = np.random.randn(7,3)        # 重み(標準正規分布に従ってランダムに生成)\n",
    "h = np.dot(c, W)                # 中間ノード\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.50928377 -2.90430582 -1.996445  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 行列積のレイヤ\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "# MatMulレイヤを使って変換する例\n",
    "c = np.array([[1,0,0,0,0,0,0]]) # 入力\n",
    "W = np.random.randn(7,3)        # 重み\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)            # 中間ノード\n",
    "print(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 シンプルな word2vec\n",
    "* ニューラルネットワークを使用して（コンテキストから各単語の出現確率を出力する）モデルを設計する\n",
    "* モデルとしてCBOW(continuous bag-of-words)を使う\n",
    "### 3.2.1 CBOWモデルの推論処理\n",
    "* CBOWモデルはコンテキスト(周辺の単語)からターゲット(中央の単語)を推論するためのニューラルネットワーク\n",
    "  * 入力はコンテキスト，つまり(one-hot表現で著された)単語のリスト\n",
    "  * コンテキストとして何語を考えるかで入力層が変わってくる．ここでは2つを考える\n",
    "  * 中間層の値は各入力層と重みとの行列積の平均をとる\n",
    "    * **この重みが単語の分散表現となる**\n",
    "    * **中間層のニューロン数を入力層よりも少なくする**．こうすることで密な表現が得られることになる．\n",
    "    * 入力層から中間層への変換を「エンコード」と呼ぶ\n",
    "  * 中間層から全結合層を経て出力層が計算される．これは行列積の計算に相当する\n",
    "    * 中間層から出力層への変換を「デコード」と呼ぶ\n",
    "  * 出力層は単語の語数だけのニューロンからなり，その値が高いほど高い出現確率を表す．Softmax関数を適用することで確率として解釈される値も得られる\n",
    "* 学習を重ねることでコンテキストから出現する単語をうまく推測できるようになる\n",
    "  * 得られたベクトルには単語の意味もエンコードされる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32324315 -0.34171576 -0.01580541 -0.01017029 -0.19490854  0.01232151\n",
      "  -0.13027292]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "# 入力層の2つの単語\n",
    "c0 = np.array([[1,0,0,0,0,0,0]])\n",
    "c1 = np.array([[0,0,1,0,0,0,0]])\n",
    "\n",
    "# 重み行列\n",
    "W_in = np.random.randn(7,3)\n",
    "W_out = np.random.randn(3,7)\n",
    "\n",
    "# MatMulレイヤ\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)    # 2つの入力層はレイヤを共有する\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 推論処理\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h) # スコア(Softmax適用前）\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 CBOWモデルの学習\n",
    "* これまでのCBOWモデルでは出力層で各単語のスコアを出力したが，これにSoftmaxを適用することで確率を得ることができる\n",
    "  * CBOWモデルはコーパスにおける単語の出現パターンを学ぶだけなので，**コーパスが変われば単語の分散表現も変わる**\n",
    "* 学習においては，Softmax関数の出力と教師ラベルとから交差エントロピー誤差を求め，それを損失として学習を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 word2vecの重みと分散表現\n",
    "* word2vecで使用するネットワークには入力側と出力側とで2つの重みがある\n",
    "* どちらも単語の分散表現が格納しているが，どちらを用いるべきか\n",
    "* word2vec(特に skip-gramモデル)では**入力側のみを使う**のがポピュラー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 学習データの準備\n",
    "### 3.3.1 コンテキストとターゲット\n",
    "* コーパスからコンテキストとターゲットとを自動生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus = [0 1 2 3 4 1 5 6]\n",
      "id_to_word = {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 1, 5, 6]),\n",
       " {'.': 6, 'and': 3, 'goodbye': 2, 'hello': 5, 'i': 4, 'say': 1, 'you': 0},\n",
       " {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .') # ピリオドも分割するため\n",
    "    words = text.split(' ') # 空白で分割\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words: # 単語ごとに\n",
    "        if word not in word_to_id: # 含まれていなければ\n",
    "            new_id = len(word_to_id) # IDを付与(0スタート)\n",
    "            word_to_id[word] = new_id # (単語, ID)の組を辞書に登録\n",
    "            id_to_word[new_id] = word # (ID, 単語)の組を辞書に登録\n",
    "\n",
    "    # 元のテキストをIDの並びに置き換えたものをコーパスとする\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "def do_preprocess(text, corpus, word_to_id, id_to_word):\n",
    "    corpus, word_to_id, id_to_word = preprocess(text)\n",
    "    print(\"corpus = {c}\".format(c=corpus))\n",
    "    print(\"id_to_word = {itw}\".format(itw=id_to_word))\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "do_preprocess(text, corpus, word_to_id, id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus = [0 1 2 3 4 1 5 6]\n",
      "id_to_word = {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "contexts = [[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "target = [1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''コーパスからターゲットとコンテキストのリストをそれぞれ生成する\n",
    "\n",
    "    :param corpus: 元のテキストをIDの並びに置き換えたもの(単語IDのNumPy配列)\n",
    "    :param window_size: ターゲット周囲の何語をコンテキストにするかのサイズ\n",
    "    :return: ターゲット(ID)のリストおよびコンテキストのリスト(共にNumPy配列)\n",
    "    '''\n",
    "    # 両端はターゲットにできないのでそれを除く部分をターゲットに設定\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    # ターゲットとなる単語それぞれに対するコンテキストをかき集める\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        # -window_size, ...　, 0, ... , window_size の範囲で\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0: # 0の場合を除く\n",
    "                continue\n",
    "            # ターゲット(idx)の周囲の単語(idx+t)をcsに追加\n",
    "            cs.append(corpus[idx + t])\n",
    "        # ターゲット(idx)のコンテキスト(cs)をcontexts に追加\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "def do_create_contexts_target(corpus, contexts, target):\n",
    "    \n",
    "    # corpusからcontexts とtargetを生成する\n",
    "    contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "    print(\"contexts = {ctx}\".format(ctx=contexts))\n",
    "    print(\"target = {t}\".format(t=target))\n",
    "    \n",
    "    return contexts, target\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = do_preprocess(text, corpus, word_to_id, id_to_word)\n",
    "contexts, target = do_create_contexts_target(corpus, contexts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# 配列の動作確認\n",
    "test_arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "test_target = test_arr[1:-1]\n",
    "print(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:1\n",
      " t:-1\n",
      " t:0\n",
      " t:1\n",
      "\n",
      "i:2\n",
      " t:-1\n",
      " t:0\n",
      " t:1\n",
      "\n",
      "i:3\n",
      " t:-1\n",
      " t:0\n",
      " t:1\n",
      "\n",
      "i:4\n",
      " t:-1\n",
      " t:0\n",
      " t:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rangeの動作確認\n",
    "test_winsize = 1\n",
    "test_corpsize = 6\n",
    "for i in range(test_winsize, test_corpsize-test_winsize):\n",
    "    print(\"i:{i}\".format(i=i))\n",
    "    for t in range(-test_winsize, test_winsize+1):\n",
    "        print(\" t:{t}\".format(t=t))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 one-hot 表現への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus = [0 1 2 3 4 1 5 6]\n",
      "id_to_word = {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "contexts = [[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "target = [1 2 3 4 1 5]\n",
      "corpus.shape=(6,)\n",
      "N=6\n",
      "corpus.shape=(6, 7)\n",
      "N=6\n",
      "C=7\n",
      "contexts = \n",
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0]]]\n",
      "target = \n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''one-hot表現への変換\n",
    "\n",
    "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
    "    :param vocab_size: 語彙数\n",
    "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
    "    '''\n",
    "\n",
    "    # N:コーパスの語数\n",
    "    print(\"corpus.shape={s}\".format(s=corpus.shape))\n",
    "    N = corpus.shape[0]\n",
    "    print(\"N={n}\".format(n=N))\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        # サイズが N行 vocab_size列で，値が全て0の配列を作る\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "\n",
    "        # idx行 word_id列を1にする\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        print(\"C={c}\".format(c=C))\n",
    "        # サイズが(N,C,vocan_size)で値が全て0の配列を作る\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        \n",
    "        # (idx_0, idx_1, word_id) の値を1にする\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "    return one_hot\n",
    "\n",
    "def do_convert_one_hot(contexts, target):\n",
    "    # 単語ID表現をone-hotに変換する\n",
    "    vocab_size = len(word_to_id)\n",
    "    target = convert_one_hot(target, vocab_size)\n",
    "    contexts = convert_one_hot(target, vocab_size)\n",
    "    \n",
    "    print(\"contexts = \\n{ctx}\".format(ctx=contexts))\n",
    "    print(\"target = \\n{t}\".format(t=target))\n",
    "    \n",
    "    return contexts, target\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = do_preprocess(text, corpus, word_to_id, id_to_word)\n",
    "contexts, target = do_create_contexts_target(corpus, contexts, target)\n",
    "contexts, target = do_convert_one_hot(contexts, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 CBOWモデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    # 初期化\n",
    "    # vocab_size: 語彙数\n",
    "    # hidden_size: 隠れ層のニューロン数\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化(重みは浮動小数点数の値をとる)\n",
    "        # randn により標準正規分布に従ってランダムに初期値を生成\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 学習コードの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソフトマックス関数\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftmaxWithLossレイヤ\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmaxの出力\n",
    "        self.t = None  # 教師ラベル\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 勾配を求め、パラメータを更新\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 評価\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    パラメータ配列中の重複する重みをひとつに集約し、\n",
    "    その重みに対応する勾配を加算する\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 重みを共有する場合\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 勾配の加算\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 転置行列として重みを共有する場合（weight tying）\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.shape=(6,)\n",
      "N=6\n",
      "corpus.shape=(6, 2)\n",
      "N=6\n",
      "C=2\n",
      "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 2 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 3 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 4 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 5 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 6 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 7 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 8 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 9 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 10 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 11 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 12 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 13 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 14 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 15 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 16 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 17 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 18 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 19 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 20 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 21 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 22 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 23 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 24 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 25 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 26 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 27 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 28 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 29 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 30 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 31 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 32 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 33 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 34 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 35 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 36 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 37 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 38 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 39 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 40 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 41 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 42 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 43 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 44 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 45 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 46 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 47 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 48 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 49 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 50 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 51 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 52 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 53 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 54 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 55 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 56 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 57 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 58 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 59 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 60 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 61 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 62 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 63 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 64 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 65 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 66 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 67 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 68 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 69 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 70 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 71 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 72 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 73 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 74 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 75 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
      "| epoch 76 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
      "| epoch 77 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
      "| epoch 78 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 79 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 80 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 81 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 82 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 83 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 84 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 85 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 86 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 87 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 88 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 89 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 90 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 91 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 92 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
      "| epoch 93 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 94 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 95 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 96 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 97 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 98 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 99 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 100 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 102 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
      "| epoch 103 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
      "| epoch 104 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
      "| epoch 105 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 106 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 107 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 108 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 109 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 110 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 111 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 112 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 113 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
      "| epoch 114 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
      "| epoch 115 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 116 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 117 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
      "| epoch 118 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
      "| epoch 119 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
      "| epoch 120 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 121 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 122 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 123 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
      "| epoch 124 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 125 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 126 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
      "| epoch 127 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 128 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 129 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 130 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 131 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 132 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 133 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 134 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 135 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 136 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
      "| epoch 137 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 138 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 139 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
      "| epoch 140 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 141 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 142 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 143 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 144 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 145 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 146 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 147 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 148 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 149 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 150 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 151 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 152 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 153 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
      "| epoch 154 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 155 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 156 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 157 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 158 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 159 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 160 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 161 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 162 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 163 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
      "| epoch 164 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 165 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 166 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 167 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 168 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 169 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
      "| epoch 170 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 171 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 172 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
      "| epoch 173 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 174 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 175 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
      "| epoch 176 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
      "| epoch 177 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 178 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 179 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 180 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 181 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
      "| epoch 182 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 183 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 184 |  iter 1 / 2 | time 0[s] | loss 1.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 185 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 186 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 187 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 188 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 189 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 190 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 191 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 192 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 193 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 194 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 195 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 196 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 197 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 198 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 199 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 200 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 202 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 203 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 204 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 205 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 206 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 207 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 208 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 209 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 210 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 211 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 212 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 213 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 214 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 215 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 216 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 217 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 218 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 219 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 220 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 221 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 222 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 223 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 224 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 225 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 226 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 227 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 228 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 229 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 230 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 231 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 232 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 233 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 234 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 235 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 236 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 237 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 238 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 239 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 240 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 241 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 242 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 243 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 244 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 245 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 246 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 247 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 248 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 249 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 250 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 251 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 252 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 253 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 254 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 255 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 256 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 257 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 258 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 259 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 260 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 261 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 262 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 263 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 264 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 265 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 266 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 267 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 268 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 269 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 270 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 271 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 272 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 273 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 274 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 275 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 276 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 277 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 278 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 279 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 280 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 281 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 282 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 283 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 284 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 285 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 286 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 287 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 288 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 289 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 290 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 291 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 292 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 293 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 294 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 295 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 296 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 297 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 298 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 299 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 300 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 302 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 303 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 304 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 305 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 306 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 307 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 308 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 309 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 310 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 311 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 312 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 313 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 314 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 315 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 316 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 317 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 318 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 319 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 320 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 321 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 322 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 323 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 324 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 325 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 326 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 327 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 328 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 329 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 330 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 331 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 332 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 333 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 334 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 335 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 336 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 337 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 338 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 339 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 340 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 341 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 342 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 343 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 344 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 345 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 346 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 347 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 348 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 349 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 350 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 351 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 352 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 353 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 354 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 355 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 356 |  iter 1 / 2 | time 0[s] | loss 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 357 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 358 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 359 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 360 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 361 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 362 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 363 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 364 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 365 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 366 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 367 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 368 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 369 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 370 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 371 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 372 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 373 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 374 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 375 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 376 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 377 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 378 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 379 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 380 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 381 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 382 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 383 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 384 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 385 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 386 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 387 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 388 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 389 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 390 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 391 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 392 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 393 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 394 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 395 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 396 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 397 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 398 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 399 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 400 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 402 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 403 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 404 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 405 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 406 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 407 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 408 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 409 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 410 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 411 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 412 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 413 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 414 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 415 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 416 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 417 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 418 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 419 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 420 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 421 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 422 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 423 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 424 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 425 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 426 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 427 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 428 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 429 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 430 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 431 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 432 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 433 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 434 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 435 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 436 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 437 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 438 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 439 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 440 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 441 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 442 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 443 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 444 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 445 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 446 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 447 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 448 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 449 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 450 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 451 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 452 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 453 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 454 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 455 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 456 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 457 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 458 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 459 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 460 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 461 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 462 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 463 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 464 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 465 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 466 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 467 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 468 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 469 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 470 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 471 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 472 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 473 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 474 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 475 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 476 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 477 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 478 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 479 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 480 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 481 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 482 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 483 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 484 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 485 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 486 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 487 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 488 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 489 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 490 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 491 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 492 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 493 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 494 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 495 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 496 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 497 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 498 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 499 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 500 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 502 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 503 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 504 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 505 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 506 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 507 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 508 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 509 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 510 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 511 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 512 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 513 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 514 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 515 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 516 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 517 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 518 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 519 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 520 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 521 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 522 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 523 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 524 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 525 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 526 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 527 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 528 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 529 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 530 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 531 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 532 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 533 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 534 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 535 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 536 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 537 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 538 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 539 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 540 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 541 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 542 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 543 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 544 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 545 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 546 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 547 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 548 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 549 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 550 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 551 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 552 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 553 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 554 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 555 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 556 |  iter 1 / 2 | time 0[s] | loss 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 557 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 558 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 559 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 560 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 561 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 562 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 563 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 564 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 565 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 566 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 567 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 568 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 569 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 570 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 571 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 572 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 573 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 574 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 575 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 576 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 577 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 578 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 579 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 580 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 581 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 582 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 583 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 584 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 585 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 586 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 587 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 588 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 589 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 590 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 591 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 592 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 593 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 594 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 595 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 596 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 597 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 598 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 599 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 600 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 602 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 603 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 604 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 605 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 606 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 607 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 608 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 609 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 610 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 611 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 612 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 613 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 614 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 615 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 616 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 617 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 618 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 619 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 620 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 621 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 622 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 623 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 624 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 625 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 626 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 627 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 628 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 629 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 630 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 631 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 632 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 633 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 634 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 635 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 636 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 637 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 638 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 639 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 640 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 641 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 642 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 643 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 644 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 645 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 646 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 647 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 648 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 649 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 650 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 651 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 652 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 653 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 654 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 655 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 656 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 657 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 658 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 659 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 660 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 661 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 662 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 663 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 664 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 665 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 666 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 667 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 668 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 669 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 670 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 671 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 672 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 673 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 674 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 675 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 676 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 677 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 678 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 679 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 680 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 681 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 682 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 683 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 684 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 685 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 686 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 687 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 688 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 689 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 690 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 691 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 692 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 693 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 694 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 695 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 696 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 697 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 698 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 699 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 700 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 702 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 703 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 704 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 705 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 706 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 707 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 708 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 709 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 710 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 711 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 712 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 713 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 714 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 715 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 716 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 717 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 718 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 719 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 720 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 721 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 722 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 723 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 724 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 725 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 726 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 727 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 728 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 729 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 730 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 731 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 732 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 733 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 734 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 735 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 736 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 737 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 738 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 739 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 740 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 741 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 742 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 743 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 744 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 745 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 746 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 747 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 748 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 749 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 750 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 751 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 752 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 753 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 754 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 755 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 756 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 757 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 758 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 759 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 760 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 761 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 762 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 763 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 764 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 765 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 766 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 767 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 768 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 769 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 770 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 771 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 772 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 773 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 774 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 775 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 776 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 777 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 778 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 779 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 780 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 781 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 782 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 783 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 784 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 785 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 786 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 787 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 788 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 789 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 790 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 791 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 792 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 793 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 794 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 795 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 796 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 797 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 798 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 799 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 800 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 802 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 803 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 804 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 805 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 806 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 807 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 808 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 809 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 810 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 811 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 812 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 813 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 814 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 815 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 816 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 817 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 818 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 819 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 820 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 821 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 822 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 823 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 824 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 825 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 826 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 827 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 828 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 829 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 830 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 831 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 832 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 833 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 834 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 835 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 836 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
      "| epoch 837 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
      "| epoch 838 |  iter 1 / 2 | time 1[s] | loss 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 839 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
      "| epoch 840 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
      "| epoch 841 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 842 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
      "| epoch 843 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
      "| epoch 844 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 845 |  iter 1 / 2 | time 1[s] | loss 1.02\n",
      "| epoch 846 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 847 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
      "| epoch 848 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 849 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
      "| epoch 850 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
      "| epoch 851 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
      "| epoch 852 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
      "| epoch 853 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
      "| epoch 854 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 855 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
      "| epoch 856 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
      "| epoch 857 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 858 |  iter 1 / 2 | time 1[s] | loss 0.92\n",
      "| epoch 859 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 860 |  iter 1 / 2 | time 1[s] | loss 1.12\n",
      "| epoch 861 |  iter 1 / 2 | time 1[s] | loss 0.78\n",
      "| epoch 862 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 863 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
      "| epoch 864 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
      "| epoch 865 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 866 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
      "| epoch 867 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 868 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 869 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
      "| epoch 870 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
      "| epoch 871 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
      "| epoch 872 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 873 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 874 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 875 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
      "| epoch 876 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
      "| epoch 877 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 878 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 879 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 880 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
      "| epoch 881 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
      "| epoch 882 |  iter 1 / 2 | time 1[s] | loss 1.10\n",
      "| epoch 883 |  iter 1 / 2 | time 1[s] | loss 0.62\n",
      "| epoch 884 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
      "| epoch 885 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
      "| epoch 886 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
      "| epoch 887 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 888 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
      "| epoch 889 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 890 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 891 |  iter 1 / 2 | time 1[s] | loss 0.91\n",
      "| epoch 892 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 893 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 894 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
      "| epoch 895 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
      "| epoch 896 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 897 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
      "| epoch 898 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
      "| epoch 899 |  iter 1 / 2 | time 1[s] | loss 0.61\n",
      "| epoch 900 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
      "| epoch 901 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
      "| epoch 902 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
      "| epoch 903 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
      "| epoch 904 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
      "| epoch 905 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 906 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 907 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 908 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
      "| epoch 909 |  iter 1 / 2 | time 1[s] | loss 0.74\n",
      "| epoch 910 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 911 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 912 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 913 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 914 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 915 |  iter 1 / 2 | time 1[s] | loss 0.95\n",
      "| epoch 916 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 917 |  iter 1 / 2 | time 1[s] | loss 0.61\n",
      "| epoch 918 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
      "| epoch 919 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
      "| epoch 920 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 921 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
      "| epoch 922 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
      "| epoch 923 |  iter 1 / 2 | time 1[s] | loss 0.74\n",
      "| epoch 924 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 925 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 926 |  iter 1 / 2 | time 1[s] | loss 0.59\n",
      "| epoch 927 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
      "| epoch 928 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 929 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 930 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 931 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 932 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 933 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
      "| epoch 934 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
      "| epoch 935 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
      "| epoch 936 |  iter 1 / 2 | time 1[s] | loss 0.95\n",
      "| epoch 937 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 938 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 939 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
      "| epoch 940 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 941 |  iter 1 / 2 | time 1[s] | loss 0.94\n",
      "| epoch 942 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
      "| epoch 943 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
      "| epoch 944 |  iter 1 / 2 | time 1[s] | loss 0.81\n",
      "| epoch 945 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 946 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 947 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
      "| epoch 948 |  iter 1 / 2 | time 1[s] | loss 0.94\n",
      "| epoch 949 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
      "| epoch 950 |  iter 1 / 2 | time 1[s] | loss 0.93\n",
      "| epoch 951 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 952 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 953 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 954 |  iter 1 / 2 | time 1[s] | loss 0.59\n",
      "| epoch 955 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 956 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 957 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
      "| epoch 958 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
      "| epoch 959 |  iter 1 / 2 | time 1[s] | loss 0.95\n",
      "| epoch 960 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
      "| epoch 961 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 962 |  iter 1 / 2 | time 1[s] | loss 0.81\n",
      "| epoch 963 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
      "| epoch 964 |  iter 1 / 2 | time 1[s] | loss 0.93\n",
      "| epoch 965 |  iter 1 / 2 | time 1[s] | loss 0.63\n",
      "| epoch 966 |  iter 1 / 2 | time 1[s] | loss 0.92\n",
      "| epoch 967 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
      "| epoch 968 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 969 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 970 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
      "| epoch 971 |  iter 1 / 2 | time 1[s] | loss 0.57\n",
      "| epoch 972 |  iter 1 / 2 | time 1[s] | loss 1.07\n",
      "| epoch 973 |  iter 1 / 2 | time 1[s] | loss 0.60\n",
      "| epoch 974 |  iter 1 / 2 | time 1[s] | loss 0.95\n",
      "| epoch 975 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
      "| epoch 976 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 977 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
      "| epoch 978 |  iter 1 / 2 | time 1[s] | loss 0.80\n",
      "| epoch 979 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
      "| epoch 980 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 981 |  iter 1 / 2 | time 1[s] | loss 0.68\n",
      "| epoch 982 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
      "| epoch 983 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 984 |  iter 1 / 2 | time 1[s] | loss 0.80\n",
      "| epoch 985 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 986 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
      "| epoch 987 |  iter 1 / 2 | time 1[s] | loss 0.68\n",
      "| epoch 988 |  iter 1 / 2 | time 1[s] | loss 0.94\n",
      "| epoch 989 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
      "| epoch 990 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 991 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
      "| epoch 992 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 993 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
      "| epoch 994 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 995 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
      "| epoch 996 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 997 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
      "| epoch 998 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
      "| epoch 999 |  iter 1 / 2 | time 1[s] | loss 0.82\n",
      "| epoch 1000 |  iter 1 / 2 | time 1[s] | loss 1.08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10df87e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-1.1448408  1.154443  -1.1890558  1.1675558  1.2287627]\n",
      "say [ 1.1712828 -1.1685073  1.2199197 -1.1945546 -1.0329347]\n",
      "goodbye [-0.7703358   0.7808129  -0.72636896  0.7417283   0.73464614]\n",
      "and [ 1.0438137 -1.1115834  0.7991398 -1.0349817 -1.5849347]\n",
      "i [-0.76234543  0.7868732  -0.74084765  0.7248235   0.7560703 ]\n",
      "hello [-1.1322765  1.1411966 -1.1688336  1.1920581  1.2297972]\n",
      ". [ 0.9425348  -0.89114755  1.3157017  -1.0557013   0.3425734 ]\n"
     ]
    }
   ],
   "source": [
    "# これまでのまとめ\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "# テキストからコーパスを作成\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "# コーパスからターゲットおよびコンテキスト（いずれもone-hot表現）を生成\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "# CBOWモデルの生成\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "\n",
    "# 学習用クラスの生成(最適化手法：Adam)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 学習\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "# 結果の表示\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 word2vecに関する補足\n",
    "### 3.5.1 CBOWモデルと確率\n",
    "\n",
    "#### 確率の表記について\n",
    "* 確率の表現\n",
    "  * $P(A)$　：Aという事象が起こる確率\n",
    "  * $P(A,B)$　：AとBが同時に起こる確率（**同時確率**）\n",
    "  * $P(A \\mid B)$　：Bが与えられた時にAが起こる確率(**事後確率**)\n",
    "\n",
    "#### CBOWの確率表現\n",
    "\n",
    "CBOWをモデル化すると次式で表現できる\n",
    "* 「コンテキスト $w_{t-1}$ と $w_{t+1}$ が与えられた時にターゲット $w_t$ が起こる確率」\n",
    "\n",
    "$$P(w_t \\mid w_{t-1}w_{t+1})$$\n",
    "\n",
    "CBOWモデルの損失関数を「交差エントロピー誤差」を用いて導出する\n",
    "\n",
    "交差エントロピー誤差\n",
    "\n",
    "$$L = -\\sum_{k}t_k\\log{y_k}$$\n",
    "\n",
    "ここで，\n",
    "* $y_k$：$k$ 番目に対応する事象が起こる確率 $P(w_k \\mid w_{k-1}w_{k+1})$\n",
    "* $t_k$：教師ラベル(one-hot表現の場合は (0,0,...,0,1,0,...,0)のような形)\n",
    "\n",
    "$t_k$ がone-hotベクトルで表現され，k番目の要素だけ1でそれ以外は0であるため，それを考慮すると，$y_k$のk番目の要素だけが使われることになるため，次のようになる\n",
    "\n",
    "**（上の交差エントロピー誤差では教師ラベルの記号に$t$を使っているため$k$番目の要素に注目しているが，下の式では$t$番目の要素$y_t$を取り出す話にさりげなく変わっていることに注意すること）**\n",
    "\n",
    "$$L = -\\log{P(w_t \\mid w_{t-1}, w_{t+1})}$$\n",
    "\n",
    "つまり，CBOWモデルの損失関数は確率$P(w_t \\mid w_{t-1}w_{t+1})$に対して $\\log$ をとり，負号をつけたものになる\n",
    "* **負の対数尤度(negative log likelihood)**という\n",
    "\n",
    "これをコーパス全体に拡張すると次のようになる\n",
    "\n",
    "$$L = -\\frac{1}{T}\\sum_{t=1}^{T}\\log{P(w_t \\mid w_{t-1},w_{t+1})}$$\n",
    "\n",
    "#### CBOWモデルの学習\n",
    "\n",
    "* CBOWモデルの学習で行うことは，上記の損失関数を小さくすること\n",
    "* 学習の結果得られる重みパラメータが単語の分散表現となる\n",
    "  * 単語がone-hotで表現されることからIDがtの単語は行列のt行目だけを抜き出すことになるという点に注意する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 skip-gram モデル\n",
    "#### CBOWモデルとski-gramモデル\n",
    "* CBOWモデルはコンテキスト(周囲の単語)からターゲット(中央の単語)を推測する\n",
    "* skip-gramはターゲットからコンテキストを推測する\n",
    "\n",
    "#### skip-gram モデルの構造\n",
    "* 入力層は一つ\n",
    "* 出力がコンテキストの数だけ存在\n",
    "  * 損失をそれぞれのコンテキストの分だけ計算する\n",
    "    * コンテキストが4つからなるならば4つの損失を計算する\n",
    "  * それぞれの損失の和を最終的な損失とする\n",
    "\n",
    "#### 確率表記\n",
    "skip-gram をモデル化すると次式で表現できる\n",
    "* つまり「$w_t$が与えられた時に$w_{t-1}$と$w_{t+1}$とが同時に起こる確率」\n",
    "\n",
    "$$P(w_{t-1},w_{t+1} \\mid w_t)$$\n",
    "\n",
    "skip-gramモデルではコンテキストの単語間に関連性がない(条件付き独立)と仮定する\n",
    "\n",
    "$$P(w_{t-1}m w_{t+1} \\mid w_t) = P(w_{t-1} \\mid w_t)P(w_{t+1} \\mid w_t)$$\n",
    "\n",
    "これを交差エントロピー誤差に適用して損失関数を導出する\n",
    "\n",
    "$$L = -\\log{P(w_{t-1},w_{t+1}\\mid w_t)}$$\n",
    "\n",
    "$$ = -\\log{P(w_{t-1}\\mid w_t)P(w_{t+1}\\mid w_t)}$$\n",
    "\n",
    "$$ = -(\\log{P(w_{t-1}\\mid w_t)}+\\log{P(w_{t+1}\\mid w_t)})$$\n",
    "\n",
    "skip-gramモデルの損失関数は全ての $t$ に対するコンテキストの損失を合計したものになるため，次のようになる\n",
    "\n",
    "$$L = -\\frac{1}{T}\\sum_{t=1}^{T}(\\log{P(w_{t-1}\\mid w_t)}+\\log{P(w_{t+1}\\mid w_t)})$$\n",
    "\n",
    "#### CBOWとskip-gram\n",
    "* 手順の違い\n",
    "  * skip-gramはコンテキストの数だけ推論して損失関数を求める．そして総和を計算する\n",
    "  * CBOWは一つのターゲットの損失を求める\n",
    "* どちらを使うべきか：skip-gram\n",
    "  * 単語の分散表現の精度で skip-gram の方が良い結果を得られる\n",
    "  * コーパスが大規模になるにつ低頻出単語や類推問題の性能でskip-gramの方が優れる\n",
    "  * 学習速度ではCBOWが高速．skip-gramは損失をコンテキストの数だけ求める必要があるため"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 カウントベース v.s. 推論ベース\n",
    "* 両者の比較については[3.1.1](Chapter%203.ipynb#3.1.1-カウントベースの手法の問題点)を参照\n",
    "* 両者の関連性\n",
    "  * skip-gram と Negative Sampling を利用したモデルは，コーパス全体の共起行列に対して若干の処理および特殊な行列分解を施したものに等しい\n",
    "  * 推論ベースとカウントベースの両手法を融合させた手法も提案されている\n",
    "    * [GloVe](https://nlp.stanford.edu/projects/glove/)：コーパス全体の統計データの情報を損失関数に取り入れミニバッチ学習を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 まとめ\n",
    "* これまでやろうとしてきたことは「単語の分散表現」を得ること\n",
    "* 分散表現を得るための手法としてカウントベースの手法と推論ベースの手法がある\n",
    "  * どちらも「分布仮説」に基づき「単語の共起性」をいかにモデル化するかという点を問題にしている\n",
    "  * 推論ベースの手法は重みの再学習ができる．そのため単語の分散表現の更新や追加を効率的に行うことができる\n",
    "* 推論ベースの手法は推測することを目標とし，その副産物として単語の分散表現が得られる\n",
    "* 推論ベースの手法としてword2vecがある．\n",
    "  * word2vecはTomas Mikolov氏による一連の論文によって提案された\n",
    "    * [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "    * [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "  * word2vecの有用性は自然言語処理のタスクで示されてきた\n",
    "\n",
    "* word2vecにはskip-gramモデルとCBOWモデルとがある\n",
    "  * word2vecのモデルはシンプルな2層のニューラルネットワークで構成される\n",
    "  * 両者の違い\n",
    "    * CBOWモデルは複数の単語（コンテキスト）から一つの単語（ターゲット）を推測する\n",
    "    * skip-gramモデルは一つの単語（ターゲット）から複数の単語（コンテキスト）を推測する\n",
    "  * 本章ではword2vecのCBOWモデルと呼ばれるニューラルネットワークについて説明\n",
    "    * CBOWモデルの構築には MatMulレイヤとSoftmax with Lossレイヤを使った\n",
    "    * CBOWモデルを小規模なコーパスで学習できることを確認した\n",
    "* 次章\n",
    "  * 現状のCBOWモデルは処理効率の点で問題があるため，モデルを改良する\n",
    "  * word2vecの重要性，特に転移学習の有用性，について扱う\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
